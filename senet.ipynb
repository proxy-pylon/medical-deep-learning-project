{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/proxy-pylon/medical-deep-learning-project.git\n",
        "%cd medical-deep-learning-project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rG4k1mXeHzv",
        "outputId": "bb761c0c-4362-44d3-d496-25a701dced20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'medical-deep-learning-project'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (89/89), done.\u001b[K\n",
            "remote: Total 92 (delta 34), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (92/92), 1.44 MiB | 18.23 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/content/medical-deep-learning-project/medical-deep-learning-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfVkn0uw45i4",
        "outputId": "a5f4796b-6337-4f0a-ad1c-139b23f8b635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.23.0+cu126)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (1.6.1)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 10)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 10)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 10)) (3.6.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations->-r requirements.txt (line 11)) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations->-r requirements.txt (line 11)) (2.11.10)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations->-r requirements.txt (line 11)) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations->-r requirements.txt (line 11)) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 11)) (4.2.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 11)) (6.5.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 11)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 11)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision, platform\n",
        "print(\"torch:\", torch.__version__, \"cuda:\", torch.version.cuda)\n",
        "print(\"torchvision:\", torchvision.__version__)\n",
        "print(\"is_cuda_available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"compute capability:\", torch.cuda.get_device_capability(0))\n",
        "\n",
        "print(\"To disable GPU and force CPU usage, write this in your terminal before running the main script:\")\n",
        "print('export CUDA_VISIBLE_DEVICES=\\\"\\\"')  # for Linux/Mac\n",
        "print('set CUDA_VISIBLE_DEVICES=')        # for Windows"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSrdG3z75W64",
        "outputId": "5e1f879e-07ca-4150-875c-3e42b75b934f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch: 2.8.0+cu126 cuda: 12.6\n",
            "torchvision: 0.23.0+cu126\n",
            "is_cuda_available: True\n",
            "device: Tesla T4\n",
            "compute capability: (7, 5)\n",
            "To disable GPU and force CPU usage, write this in your terminal before running the main script:\n",
            "export CUDA_VISIBLE_DEVICES=\"\"\n",
            "set CUDA_VISIBLE_DEVICES=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpIwwn4l65dV",
        "outputId": "057aa822-2962-4c26-ce27-62efb8c55b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.10.5)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kmader/skin-cancer-mnist-ham10000\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpdOvHoJ7-GZ",
        "outputId": "bd52b381-a97d-472f-970b-13b812cb14c2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'skin-cancer-mnist-ham10000' dataset.\n",
            "Path to dataset files: /kaggle/input/skin-cancer-mnist-ham10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/root/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pi8J-EQwxHp",
        "outputId": "137b4ef7-c739-4524-da37-f6017c358901"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '/root/.cache/kagglehub/datasets/kmader/skin-cancer-mnist-ham10000/versions/2': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "# Third-party libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from numpy.typing import ArrayLike\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    brier_score_loss,\n",
        "    log_loss,\n",
        "    precision_recall_curve\n",
        ")\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision.models import ResNet50_Weights, EfficientNet_B0_Weights\n",
        "\n",
        "# Warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Add this to your imports section\n",
        "class SEModule(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation module\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class SEBasicBlock(nn.Module):\n",
        "    \"\"\"Basic ResNet block with SE module\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n",
        "        super(SEBasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.se = SEModule(planes, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class SEBottleneck(nn.Module):\n",
        "    \"\"\"Bottleneck ResNet block with SE module\"\"\"\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n",
        "        super(SEBottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.se = SEModule(planes * 4, reduction)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def se_resnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a SE-ResNet-50 model.\"\"\"\n",
        "    model = SEResNet(SEBottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "class SEResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=1000, reduction=16):\n",
        "        self.inplanes = 64\n",
        "        super(SEResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], reduction=reduction)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, reduction=reduction)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, reduction=reduction)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, reduction=reduction)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, reduction=16):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                         kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, reduction))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, reduction=reduction))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Container for training, model, and path configuration.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    HAM10000_BASE : str\n",
        "        Base directory for the HAM10000 dataset.\n",
        "    ISIC_BASE : str\n",
        "        Base directory for the ISIC dataset.\n",
        "    OUTPUT_DIR : str\n",
        "        Base output directory.\n",
        "    CHECKPOINT_DIR : str\n",
        "        Directory for checkpoints.\n",
        "    RESULTS_DIR : str\n",
        "        Directory for results and plots.\n",
        "    MODEL_NAME : str\n",
        "        Backbone model identifier. One of {'resnet50', 'efficientnet', 'vgg16'}.\n",
        "    IMG_SIZE : int\n",
        "        Input image size (square).\n",
        "    NUM_CLASSES : int\n",
        "        Number of output classes.\n",
        "    PRETRAINED : bool\n",
        "        Whether to use ImageNet pretrained weights.\n",
        "    BATCH_SIZE : int\n",
        "        Training batch size.\n",
        "    NUM_EPOCHS : int\n",
        "        Maximum number of epochs.\n",
        "    LEARNING_RATE : float\n",
        "        Base learning rate (not used directly when discriminative LRs are applied).\n",
        "    WEIGHT_DECAY : float\n",
        "        Weight decay for optimizers.\n",
        "    EARLY_STOPPING_PATIENCE : int\n",
        "        Patience for early stopping.\n",
        "    FREEZE_EPOCHS : int\n",
        "        Number of warmup epochs training the head only.\n",
        "    HEAD_LR_WARMUP : float\n",
        "        Learning rate for head during warmup.\n",
        "    HEAD_LR_FINETUNE : float\n",
        "        Learning rate for head during fine-tuning.\n",
        "    BACKBONE_LR_LOW : float\n",
        "        LR for earliest backbone layers.\n",
        "    BACKBONE_LR_MID : float\n",
        "        LR for mid backbone layers.\n",
        "    BACKBONE_LR_HIGH : float\n",
        "        LR for deepest backbone layers.\n",
        "    TEST_SIZE : float\n",
        "        Proportion for test split.\n",
        "    VAL_SIZE : float\n",
        "        Proportion for validation split (from the non-train part).\n",
        "    RANDOM_STATE : int\n",
        "        Random seed for splitting.\n",
        "    USE_MIXUP : bool\n",
        "        Placeholder flag for mixup usage.\n",
        "    USE_CUTMIX : bool\n",
        "        Placeholder flag for cutmix usage.\n",
        "    DEVICE : str\n",
        "        'cuda' if available else 'cpu'.\n",
        "    NUM_WORKERS : int\n",
        "        DataLoader workers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Data Paths\n",
        "    HAM10000_BASE = '/kaggle/input/skin-cancer-mnist-ham10000'  # Change this path\n",
        "    ISIC_BASE = 'not defined lol'\n",
        "    # Output Paths\n",
        "    OUTPUT_DIR = './output/'\n",
        "    CHECKPOINT_DIR = OUTPUT_DIR + 'checkpoints'\n",
        "    RESULTS_DIR = OUTPUT_DIR + 'results'\n",
        "\n",
        "    # Model configurations\n",
        "    MODEL_NAME = 'senet50'  # Options: 'resnet50', 'efficientnet', 'vgg16', 'senet50'\n",
        "    IMG_SIZE = 224\n",
        "    NUM_CLASSES = 2  # Binary: melanoma vs benign\n",
        "    PRETRAINED = True\n",
        "\n",
        "    # Training configurations\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 250\n",
        "    LEARNING_RATE = 0.001\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EARLY_STOPPING_PATIENCE = 30\n",
        "\n",
        "    # Warmup / fine-tune\n",
        "    FREEZE_EPOCHS = 3\n",
        "    HEAD_LR_WARMUP = 1e-3\n",
        "    HEAD_LR_FINETUNE = 1e-4\n",
        "\n",
        "    # Discriminative LRs for backbone\n",
        "    BACKBONE_LR_LOW = 1e-5\n",
        "    BACKBONE_LR_MID = 2e-5\n",
        "    BACKBONE_LR_HIGH = 3e-5\n",
        "\n",
        "    # Dataset split ratio and seeding\n",
        "    TEST_SIZE = 0.30\n",
        "    VAL_SIZE = 0.20\n",
        "    RANDOM_STATE = 42\n",
        "\n",
        "    # Augmentation flags\n",
        "    USE_MIXUP = False\n",
        "    USE_CUTMIX = False\n",
        "\n",
        "    # Device\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "def load_ham10000_data(base_path: Union[str, os.PathLike]) -> pd.DataFrame:\n",
        "    \"\"\"Load HAM10000 metadata and resolve image paths.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    base_path : str or os.PathLike\n",
        "        Directory containing `HAM10000_metadata.csv` and image folders.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        DataFrame with columns including:\n",
        "        - 'image_id'\n",
        "        - 'dx' (diagnosis)\n",
        "        - 'image_path' (resolved path to JPEG)\n",
        "        - 'binary_label' (1 for melanoma, 0 otherwise)\n",
        "    \"\"\"\n",
        "    print('Loading HAM10000 dataset....')\n",
        "    base_path = str(base_path)\n",
        "\n",
        "    metadata_path = os.path.join(base_path, 'HAM10000_metadata.csv')\n",
        "    df = pd.read_csv(metadata_path)\n",
        "\n",
        "    def get_image_path(image_id: str) -> Optional[str]:\n",
        "        \"\"\"Return path to image file if present in either image folder.\"\"\"\n",
        "        part1 = os.path.join(base_path, 'HAM10000_images_part_1', f'{image_id}.jpg')\n",
        "        part2 = os.path.join(base_path, 'HAM10000_images_part_2', f'{image_id}.jpg')\n",
        "        if os.path.exists(part1):\n",
        "            return part1\n",
        "        if os.path.exists(part2):\n",
        "            return part2\n",
        "        return None\n",
        "\n",
        "    df['image_path'] = df['image_id'].apply(get_image_path)\n",
        "    df = df[df['image_path'].notna()].reset_index(drop=True)\n",
        "    df['binary_label'] = (df['dx'] == 'mel').astype(int)\n",
        "\n",
        "    print(f\"Loaded {len(df)} images\")\n",
        "    print(f\"Melanoma: {df['binary_label'].sum()}\")\n",
        "    print(f\"Benign: {len(df) - df['binary_label'].sum()}\")\n",
        "    print(f\"\\nClass Distribution:\")\n",
        "    print(df['dx'].value_counts())\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "class MelanomaDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for melanoma classification with Albumentations transforms.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataframe : pd.DataFrame\n",
        "        DataFrame with at least columns 'image_path' and 'binary_label'.\n",
        "    transform : Optional[A.Compose]\n",
        "        Albumentations transform to apply to the loaded image.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A sample dict containing:\n",
        "        - 'image': torch.Tensor of shape [3, H, W]\n",
        "        - 'label': torch.LongTensor scalar {0, 1}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe: pd.DataFrame, transform: Optional[A.Compose] = None) -> None:\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Number of samples.\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Load one sample by index.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx : int\n",
        "            Row index.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Dict with 'image' and 'label' tensors.\n",
        "        \"\"\"\n",
        "        img_path: str = self.df.loc[idx, 'image_path']\n",
        "        label: int = int(self.df.loc[idx, 'binary_label'])\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "def get_train_transform(img_size: int = 224) -> A.Compose:\n",
        "    \"\"\"Build the training augmentation pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int, default=224\n",
        "        Target square image size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A.Compose\n",
        "        Albumentations composition.\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.RandomResizedCrop(\n",
        "            size=(img_size, img_size),\n",
        "            scale=(0.90, 1.00),\n",
        "            ratio=(0.9, 1.1),\n",
        "            p=1.0\n",
        "        ),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Rotate(limit=20, border_mode=cv2.BORDER_REFLECT_101, p=0.7),\n",
        "        A.CenterCrop(height=img_size, width=img_size, p=1.0),\n",
        "        A.Affine(scale=(0.95, 1.05), translate_percent=(-0.02, 0.02),\n",
        "                 shear=(-5, 5), mode=cv2.BORDER_REFLECT_101, p=0.5),\n",
        "        A.RandomBrightnessContrast(0.10, 0.10, p=0.3),\n",
        "        A.ColorJitter(0.05, 0.05, 0.05, 0.02, p=0.2),\n",
        "        A.CLAHE(clip_limit=(1, 2), tile_grid_size=(8, 8), p=0.2),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(var_limit=(10.0, 50.0)),\n",
        "            A.GaussianBlur(blur_limit=(3, 7)),\n",
        "            A.MedianBlur(blur_limit=5),\n",
        "        ], p=0.3),\n",
        "        A.CoarseDropout(max_holes=1, min_holes=1,\n",
        "                        max_height=int(0.08 * img_size), max_width=int(0.08 * img_size),\n",
        "                        min_height=8, min_width=8, fill_value=0, p=0.15),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "\n",
        "def get_val_transform(img_size: int = 224) -> A.Compose:\n",
        "    \"\"\"Build the validation/test preprocessing pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int, default=224\n",
        "        Target square image size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A.Compose\n",
        "        Albumentations composition for eval.\n",
        "    \"\"\"\n",
        "    return A.Compose([\n",
        "        A.Resize(img_size, img_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "\n",
        "class MelanomaClassifier(nn.Module):\n",
        "    \"\"\"CNN classifier using a torchvision backbone with a custom head.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_name : str, default='resnet50'\n",
        "        One of {'resnet50', 'efficientnet'} currently supported.\n",
        "    num_classes : int, default=2\n",
        "        Number of output classes.\n",
        "    pretrained : bool, default=True\n",
        "        If True, initialize backbone with pretrained weights.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Forward returns both logits and the pooled backbone features.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str = 'resnet50', num_classes: int = 2, pretrained: bool = True) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if model_name == 'resnet50':\n",
        "            self.backbone = models.resnet50(\n",
        "                weights=ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
        "            )\n",
        "            num_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "\n",
        "        elif model_name == 'efficientnet':\n",
        "            self.backbone = models.efficientnet_b0(\n",
        "                weights=EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "            )\n",
        "            num_features = self.backbone.classifier[1].in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "\n",
        "        elif model_name == 'senet50':\n",
        "            self.backbone = se_resnet50(pretrained=False)  #no pretrained weights available by default\n",
        "            num_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model_name: {model_name}\")\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        features = self.backbone(x)\n",
        "        output = self.classifier(features)\n",
        "        return output, features\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Input batch of shape [N, 3, H, W].\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (torch.Tensor, torch.Tensor)\n",
        "            - logits: [N, C]\n",
        "            - features: pooled features from the backbone, shape [N, F]\n",
        "        \"\"\"\n",
        "        features = self.backbone(x)\n",
        "        output = self.classifier(features)\n",
        "        return output, features\n",
        "\n",
        "\n",
        "def set_backbone_trainable(model: nn.Module, trainable: bool) -> None:\n",
        "    \"\"\"Enable or disable gradient updates for backbone parameters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model with attribute `backbone`.\n",
        "    trainable : bool\n",
        "        If True, unfreezes the backbone; if False, freezes it.\n",
        "    \"\"\"\n",
        "    for p in model.backbone.parameters():\n",
        "        p.requires_grad = trainable\n",
        "\n",
        "\n",
        "def get_param_groups_discriminative(model: nn.Module, config: Config) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Create optimizer parameter groups with discriminative learning rates.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model with `backbone` and `classifier`.\n",
        "    config : Config\n",
        "        Hyperparameters containing LR tiers and weight decay.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of dict\n",
        "        Parameter groups consumable by torch optimizer.\n",
        "    \"\"\"\n",
        "    param_groups: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Classifier / head\n",
        "    param_groups.append({\n",
        "        \"params\": list(model.classifier.parameters()),\n",
        "        \"lr\": config.HEAD_LR_FINETUNE,\n",
        "        \"weight_decay\": config.WEIGHT_DECAY\n",
        "    })\n",
        "\n",
        "    bb = model.backbone\n",
        "    low = config.BACKBONE_LR_LOW\n",
        "    mid = config.BACKBONE_LR_MID\n",
        "    high = config.BACKBONE_LR_HIGH\n",
        "\n",
        "    if isinstance(bb, models.ResNet) or hasattr(bb, 'layer1'):  # Includes both ResNet and SENet\n",
        "        tiers = [\n",
        "            ([\"conv1\", \"bn1\"], low),\n",
        "            ([\"layer1\"], low),\n",
        "            ([\"layer2\"], mid),\n",
        "            ([\"layer3\"], high),\n",
        "            ([\"layer4\"], high),\n",
        "        ]\n",
        "        for names, lr in tiers:\n",
        "            params: List[nn.Parameter] = []\n",
        "            for n in names:\n",
        "                m = getattr(bb, n)\n",
        "                params += list(m.parameters())\n",
        "            param_groups.append({\"params\": params, \"lr\": lr, \"weight_decay\": config.WEIGHT_DECAY})\n",
        "\n",
        "    elif hasattr(bb, \"features\"):  # EfficientNet-style\n",
        "        feat = bb.features\n",
        "        n = len(feat)\n",
        "        cut1 = max(1, n // 3)\n",
        "        cut2 = max(cut1 + 1, (2 * n) // 3)\n",
        "\n",
        "        early = list(feat[:cut1].parameters())      # low\n",
        "        middle = list(feat[cut1:cut2].parameters()) # mid\n",
        "        late = list(feat[cut2:].parameters())       # high\n",
        "\n",
        "        param_groups += [\n",
        "            {\"params\": early,  \"lr\": low,  \"weight_decay\": config.WEIGHT_DECAY},\n",
        "            {\"params\": middle, \"lr\": mid,  \"weight_decay\": config.WEIGHT_DECAY},\n",
        "            {\"params\": late,   \"lr\": high, \"weight_decay\": config.WEIGHT_DECAY},\n",
        "        ]\n",
        "    else:\n",
        "        param_groups.append({\n",
        "            \"params\": [p for p in model.backbone.parameters() if p.requires_grad],\n",
        "            \"lr\": mid,\n",
        "            \"weight_decay\": config.WEIGHT_DECAY\n",
        "        })\n",
        "\n",
        "    return param_groups\n",
        "\n",
        "def build_optimizer_warmup(model: nn.Module, config: Config) -> optim.Optimizer:\n",
        "    \"\"\"Create AdamW optimizer for the head-only warmup phase.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model with a `classifier` module.\n",
        "    config : Config\n",
        "        Hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.optim.Optimizer\n",
        "        AdamW optimizer over head parameters.\n",
        "    \"\"\"\n",
        "    head_params = [p for p in model.classifier.parameters() if p.requires_grad]\n",
        "    return optim.AdamW(head_params, lr=config.HEAD_LR_WARMUP, weight_decay=config.WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "def build_optimizer_finetune(model: nn.Module, config: Config) -> optim.Optimizer:\n",
        "    \"\"\"Create AdamW optimizer with discriminative LRs for fine-tuning.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model with `backbone` and `classifier`.\n",
        "    config : Config\n",
        "        Hyperparameters.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.optim.Optimizer\n",
        "        AdamW optimizer over grouped parameters.\n",
        "    \"\"\"\n",
        "    groups = get_param_groups_discriminative(model, config)\n",
        "    return optim.AdamW(groups)\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss for addressing class imbalance.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : Optional[torch.Tensor or float], default=None\n",
        "        Class weighting factor. Either scalar or tensor of shape [num_classes].\n",
        "    gamma : float, default=2.0\n",
        "        Focusing parameter to down-weight easy examples.\n",
        "    reduction : {'none', 'mean', 'sum'}, default='mean'\n",
        "        Reduction mode.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        Reduced loss according to `reduction`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha: Optional[Union[torch.Tensor, float]] = None,\n",
        "                 gamma: float = 2.0, reduction: str = 'mean') -> None:\n",
        "        super().__init__()\n",
        "        self.alpha = alpha  # tensor of shape [num_classes] or scalar\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute focal loss.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs : torch.Tensor\n",
        "            Logits of shape [N, C].\n",
        "        targets : torch.Tensor\n",
        "            Integer class labels of shape [N].\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Loss tensor reduced by `reduction`.\n",
        "        \"\"\"\n",
        "        ce_loss = F.cross_entropy(inputs, targets, weight=self.alpha, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        if self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "\n",
        "class TemperatureScaler(nn.Module):\n",
        "    \"\"\"Post-hoc calibration via temperature scaling.\"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.log_temperature = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    @property\n",
        "    def T(self) -> torch.Tensor:\n",
        "        \"\"\"Return positive temperature parameter.\"\"\"\n",
        "        return self.log_temperature.exp()\n",
        "\n",
        "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Scale logits by temperature.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        logits : torch.Tensor\n",
        "            Unnormalized model outputs of shape [N, C].\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Scaled logits of shape [N, C].\n",
        "        \"\"\"\n",
        "        return logits / self.T\n",
        "\n",
        "\n",
        "def _nll_criterion(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Negative log-likelihood (cross-entropy) for calibration.\"\"\"\n",
        "    return F.cross_entropy(logits, targets)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_logits_and_labels(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: Union[str, torch.device]\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Run model over a DataLoader and collect logits and labels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Trained classifier.\n",
        "    loader : DataLoader\n",
        "        DataLoader yielding dicts with 'image' and 'label'.\n",
        "    device : str or torch.device\n",
        "        Computation device.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (torch.Tensor, torch.Tensor)\n",
        "        - logits: [N, C]\n",
        "        - labels: [N]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_logits: List[torch.Tensor] = []\n",
        "    all_labels: List[torch.Tensor] = []\n",
        "    for batch in tqdm(loader, desc='Collecting logits for calibration'):\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        outputs, _ = model(images)\n",
        "        all_logits.append(outputs.detach().cpu())\n",
        "        all_labels.append(labels.detach().cpu())\n",
        "    logits = torch.cat(all_logits, dim=0)\n",
        "    labels = torch.cat(all_labels, dim=0)\n",
        "    return logits, labels\n",
        "\n",
        "\n",
        "def fit_temperature(\n",
        "    model: nn.Module,\n",
        "    val_loader: DataLoader,\n",
        "    device: Union[str, torch.device],\n",
        "    max_iter: int = 200,\n",
        "    lr: float = 0.01,\n",
        "    verbose: bool = True\n",
        ") -> TemperatureScaler:\n",
        "    \"\"\"Fit a TemperatureScaler on validation data by minimizing NLL.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Classifier to calibrate.\n",
        "    val_loader : DataLoader\n",
        "        Validation loader.\n",
        "    device : str or torch.device\n",
        "        Device for computation.\n",
        "    max_iter : int, default=200\n",
        "        Max iterations for the fallback Adam optimizer.\n",
        "    lr : float, default=0.01\n",
        "        LR for fallback Adam.\n",
        "    verbose : bool, default=True\n",
        "        Print calibration summary.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    TemperatureScaler\n",
        "        Fitted temperature scaler.\n",
        "    \"\"\"\n",
        "    logits, labels = collect_logits_and_labels(model, val_loader, device)\n",
        "    scaler = TemperatureScaler().to(device)\n",
        "    optimizer = torch.optim.LBFGS(scaler.parameters(), lr=0.25, max_iter=50, line_search_fn='strong_wolfe')\n",
        "\n",
        "    logits = logits.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    def closure() -> torch.Tensor:\n",
        "        optimizer.zero_grad()\n",
        "        loss = _nll_criterion(scaler(logits), labels)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    try:\n",
        "        optimizer.step(closure)\n",
        "    except Exception:\n",
        "        opt2 = torch.optim.Adam([scaler.log_temperature], lr=lr)\n",
        "        for _ in range(max_iter):\n",
        "            opt2.zero_grad()\n",
        "            loss = _nll_criterion(scaler(logits), labels)\n",
        "            loss.backward()\n",
        "            opt2.step()\n",
        "\n",
        "    if verbose:\n",
        "        with torch.no_grad():\n",
        "            before = _nll_criterion(logits, labels).item()\n",
        "            after = _nll_criterion(scaler(logits), labels).item()\n",
        "            print(f\"Temperature learned: T={scaler.T.item():.4f} | NLL: {before:.4f} -> {after:.4f}\")\n",
        "    return scaler\n",
        "\n",
        "\n",
        "def apply_temperature(\n",
        "    logits: torch.Tensor,\n",
        "    scaler: Optional[TemperatureScaler],\n",
        "    device: Union[str, torch.device]\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Apply a fitted temperature scaler to logits if provided.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    logits : torch.Tensor\n",
        "        Raw logits [N, C].\n",
        "    scaler : Optional[TemperatureScaler]\n",
        "        Fitted scaler or None.\n",
        "    device : str or torch.device\n",
        "        Device.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        Possibly scaled logits [N, C].\n",
        "    \"\"\"\n",
        "    if scaler is None:\n",
        "        return logits\n",
        "    return scaler(logits.to(device))\n",
        "\n",
        "\n",
        "def compute_ece(probs: ArrayLike, labels: ArrayLike, n_bins: int = 15) -> float:\n",
        "    \"\"\"Compute Expected Calibration Error (ECE) for binary classification.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    probs : ArrayLike\n",
        "        Predicted probabilities for the positive class, shape [N].\n",
        "    labels : ArrayLike\n",
        "        Binary labels {0, 1}, shape [N].\n",
        "    n_bins : int, default=15\n",
        "        Number of confidence bins.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        ECE value in [0, 1].\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs)\n",
        "    labels = np.asarray(labels)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    binids = np.digitize(probs, bins[1:-1], right=True)\n",
        "\n",
        "    ece = 0.0\n",
        "    for b in range(n_bins):\n",
        "        mask = binids == b\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        conf = probs[mask].mean()\n",
        "        acc = labels[mask].mean()\n",
        "        w = mask.mean()\n",
        "        ece += w * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "\n",
        "def plot_reliability_diagram(\n",
        "    labels: ArrayLike,\n",
        "    probs: ArrayLike,\n",
        "    save_path: Union[str, os.PathLike],\n",
        "    n_bins: int = 15\n",
        ") -> None:\n",
        "    \"\"\"Plot and save a reliability diagram.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : ArrayLike\n",
        "        Binary ground-truth labels [N].\n",
        "    probs : ArrayLike\n",
        "        Predicted positive-class probabilities [N].\n",
        "    save_path : str or os.PathLike\n",
        "        Output image path.\n",
        "    n_bins : int, default=15\n",
        "        Number of bins.\n",
        "    \"\"\"\n",
        "    labels = np.asarray(labels)\n",
        "    probs = np.asarray(probs)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "    binids = np.digitize(probs, bins[1:-1], right=True)\n",
        "\n",
        "    bin_acc: List[float] = []\n",
        "    bin_conf: List[float] = []\n",
        "    for b in range(n_bins):\n",
        "        mask = binids == b\n",
        "        if not np.any(mask):\n",
        "            bin_acc.append(0.0)\n",
        "            bin_conf.append((bins[b] + bins[b + 1]) / 2.0)\n",
        "        else:\n",
        "            bin_acc.append(float(labels[mask].mean()))\n",
        "            bin_conf.append(float(probs[mask].mean()))\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', linewidth=2)\n",
        "    plt.bar(bin_conf, np.array(bin_acc) - np.array(bin_conf),\n",
        "            width=1.0 / n_bins, bottom=bin_conf, align='center', alpha=0.7)\n",
        "    plt.xlabel('Confidence')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Reliability Diagram')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: Union[str, torch.device]\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Train the model for one epoch.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model to train.\n",
        "    loader : DataLoader\n",
        "        Training DataLoader.\n",
        "    criterion : nn.Module\n",
        "        Loss function.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        Optimizer instance.\n",
        "    device : str or torch.device\n",
        "        Device.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (float, float)\n",
        "        Tuple of (average_loss, accuracy) where accuracy is in [0, 1].\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "\n",
        "    for batch in pbar:\n",
        "        images = batch['image'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': running_loss / max(1, total),\n",
        "            'acc': 100 * correct / max(1, total)\n",
        "        })\n",
        "\n",
        "    return running_loss / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    device: Union[str, torch.device]\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"Validate the model and compute loss, accuracy, and F1.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model to evaluate.\n",
        "    loader : DataLoader\n",
        "        Validation DataLoader.\n",
        "    criterion : nn.Module\n",
        "        Loss function.\n",
        "    device : str or torch.device\n",
        "        Device.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (float, float, float)\n",
        "        Tuple (avg_loss, accuracy, f1), accuracy and f1 in [0, 1].\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels: List[int] = []\n",
        "    all_preds: List[int] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Validation'):\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs, _ = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "            all_preds.extend(predicted.cpu().numpy().tolist())\n",
        "\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean().item()\n",
        "    return avg_loss, accuracy, f1\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: Union[str, torch.device],\n",
        "    scaler: Optional[TemperatureScaler] = None,\n",
        "    threshold: Optional[float] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate model with optional calibration and decision threshold.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        Model to evaluate.\n",
        "    loader : DataLoader\n",
        "        DataLoader.\n",
        "    device : str or torch.device\n",
        "        Device.\n",
        "    scaler : Optional[TemperatureScaler], default=None\n",
        "        Temperature scaler for logits.\n",
        "    threshold : Optional[float], default=None\n",
        "        If provided, use this threshold on positive-class probability.\n",
        "        If None, uses argmax over classes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Metrics and raw outputs:\n",
        "        - 'accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'ece'\n",
        "        - 'brier', 'nll'\n",
        "        - 'confusion_matrix' (np.ndarray shape [2,2])\n",
        "        - 'predictions', 'labels', 'probabilities' (lists)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds: List[int] = []\n",
        "    all_labels: List[int] = []\n",
        "    all_probs: List[float] = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc='Evaluating'):\n",
        "            images = batch['image'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits, _ = model(images)\n",
        "            if scaler is not None:\n",
        "                logits = scaler(logits)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            if threshold is None:\n",
        "                _, preds = probs.max(1)\n",
        "            else:\n",
        "                preds = (probs[:, 1] >= float(threshold)).long()\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "            all_probs.extend(probs[:, 1].cpu().numpy().tolist())\n",
        "\n",
        "    all_preds_np = np.array(all_preds)\n",
        "    all_labels_np = np.array(all_labels)\n",
        "    all_probs_np = np.array(all_probs)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels_np, all_preds_np)\n",
        "    precision = precision_score(all_labels_np, all_preds_np, zero_division=0)\n",
        "    recall = recall_score(all_labels_np, all_preds_np, zero_division=0)\n",
        "    f1 = f1_score(all_labels_np, all_preds_np, zero_division=0)\n",
        "    roc_auc = roc_auc_score(all_labels_np, all_probs_np)\n",
        "    cm = confusion_matrix(all_labels_np, all_preds_np)\n",
        "    ece = compute_ece(all_probs_np, all_labels_np)\n",
        "\n",
        "    brier = brier_score_loss(all_labels_np, all_probs_np)\n",
        "    nll = log_loss(all_labels_np, np.vstack([1 - all_probs_np, all_probs_np]).T, labels=[0, 1])\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'ece': ece,\n",
        "        'brier': brier,\n",
        "        'nll': nll,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probs\n",
        "    }\n",
        "\n",
        "\n",
        "def best_threshold_for_f1(labels: ArrayLike, probs: ArrayLike) -> Tuple[float, float]:\n",
        "    \"\"\"Compute the probability threshold that maximizes F1 on validation data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : ArrayLike\n",
        "        Binary labels {0,1}.\n",
        "    probs : ArrayLike\n",
        "        Predicted positive-class probabilities.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (float, float)\n",
        "        Tuple (threshold, best_f1).\n",
        "    \"\"\"\n",
        "    labels = np.asarray(labels)\n",
        "    probs = np.asarray(probs)\n",
        "    prec, rec, thr = precision_recall_curve(labels, probs)\n",
        "    f1 = 2 * prec[1:] * rec[1:] / (prec[1:] + rec[1:] + 1e-12)\n",
        "    idx = int(np.argmax(f1))\n",
        "    return float(thr[idx]), float(f1[idx])\n",
        "\n",
        "\n",
        "def youden_j_threshold(labels: ArrayLike, probs: ArrayLike) -> float:\n",
        "    \"\"\"Compute Youden's J statistic threshold from the ROC curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : ArrayLike\n",
        "        Binary labels {0,1}.\n",
        "    probs : ArrayLike\n",
        "        Predicted positive-class probabilities.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        Threshold that maximizes TPR - FPR.\n",
        "    \"\"\"\n",
        "    fpr, tpr, thr = roc_curve(labels, probs)\n",
        "    j = tpr - fpr\n",
        "    idx = int(np.argmax(j))\n",
        "    return float(thr[idx])\n",
        "\n",
        "\n",
        "def plot_training_history(history: Dict[str, List[float]], save_path: Union[str, os.PathLike]) -> None:\n",
        "    \"\"\"Plot training/validation loss and accuracy curves.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    history : dict\n",
        "        History dict with keys: 'train_loss', 'val_loss', 'train_acc', 'val_acc'.\n",
        "    save_path : str or os.PathLike\n",
        "        Output path for the image file.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
        "    axes[0].plot(history['val_loss'], label='Val Loss')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    axes[1].plot(history['train_acc'], label='Train Acc')\n",
        "    axes[1].plot(history['val_acc'], label='Val Acc')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].set_title('Training and Validation Accuracy')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm: np.ndarray, save_path: Union[str, os.PathLike]) -> None:\n",
        "    \"\"\"Plot and save a confusion matrix heatmap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    cm : np.ndarray\n",
        "        Confusion matrix of shape [2, 2].\n",
        "    save_path : str or os.PathLike\n",
        "        Output image path.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Benign', 'Melanoma'],\n",
        "                yticklabels=['Benign', 'Melanoma'])\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_roc_curve(\n",
        "    labels: ArrayLike,\n",
        "    probs: ArrayLike,\n",
        "    save_path: Union[str, os.PathLike]\n",
        ") -> None:\n",
        "    \"\"\"Plot and save the ROC curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labels : ArrayLike\n",
        "        Ground truth binary labels {0, 1}.\n",
        "    probs : ArrayLike\n",
        "        Predicted probabilities for the positive class.\n",
        "    save_path : str or os.PathLike\n",
        "        Path to save the ROC curve image.\n",
        "    \"\"\"\n",
        "    fpr, tpr, _ = roc_curve(labels, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main(use_ham10000: bool = True, use_isic: bool = False) -> Tuple[nn.Module, Dict[str, Any], Dict[str, List[float]]]:\n",
        "    \"\"\"Main training, calibration, and evaluation pipeline.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    use_ham10000 : bool, default=True\n",
        "        If True, load HAM10000 dataset.\n",
        "    use_isic : bool, default=False\n",
        "        If True, load ISIC dataset (requires `load_isic_data`, not provided here).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (nn.Module, dict, dict)\n",
        "        - Trained model\n",
        "        - Metrics dict for calibrated evaluation on test set\n",
        "        - Training history dict\n",
        "    \"\"\"\n",
        "    config = Config()\n",
        "    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n",
        "    os.makedirs(config.RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "    print('=' * 70)\n",
        "    print(\"MELANOMA CLASSIFICATION - TRAINING PIPELINE\")\n",
        "    print('=' * 70)\n",
        "    print(f\"Device: {config.DEVICE}\")\n",
        "    print(f\"Model: {config.MODEL_NAME}\")\n",
        "    print(f\"Image size: {config.IMG_SIZE}\")\n",
        "    print(f\"Batch size: {config.BATCH_SIZE}\")\n",
        "    print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    if use_ham10000:\n",
        "        df = load_ham10000_data(config.HAM10000_BASE)\n",
        "    elif use_isic:\n",
        "        # Placeholder: function not defined in this file.\n",
        "        df = load_isic_data(config.ISIC_BASE)  # type: ignore[name-defined]\n",
        "        df = df[df['split'] == 'train'].reset_index(drop=True)\n",
        "    else:\n",
        "        raise ValueError(\"Must specify either HAM10000 or ISIC dataset\")\n",
        "\n",
        "    print(\"\\nSplitting dataset...\")\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df, test_size=config.TEST_SIZE + config.VAL_SIZE,\n",
        "        random_state=config.RANDOM_STATE, stratify=df['binary_label']\n",
        "    )\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df, test_size=config.TEST_SIZE / (config.TEST_SIZE + config.VAL_SIZE),\n",
        "        random_state=config.RANDOM_STATE, stratify=temp_df['binary_label']\n",
        "    )\n",
        "    print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
        "\n",
        "    train_dataset = MelanomaDataset(train_df, get_train_transform(config.IMG_SIZE))\n",
        "    val_dataset = MelanomaDataset(val_df, get_val_transform(config.IMG_SIZE))\n",
        "    test_dataset = MelanomaDataset(test_df, get_val_transform(config.IMG_SIZE))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=config.NUM_WORKERS)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=config.NUM_WORKERS)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
        "                             shuffle=False, num_workers=config.NUM_WORKERS)\n",
        "\n",
        "    class_counts = train_df['binary_label'].value_counts()\n",
        "    total = len(train_df)\n",
        "    class_weights = {\n",
        "        0: total / (2 * class_counts[0]),\n",
        "        1: total / (2 * class_counts[1]) * 1.0\n",
        "    }\n",
        "    weights = torch.FloatTensor([class_weights[0], class_weights[1]]).to(config.DEVICE)\n",
        "\n",
        "    print(f\"\\nClass weights: {class_weights}\")\n",
        "\n",
        "    print(f\"\\nCreating {config.MODEL_NAME} model...\")\n",
        "    model = MelanomaClassifier(config.MODEL_NAME, config.NUM_CLASSES, config.PRETRAINED)\n",
        "    model = model.to(config.DEVICE)\n",
        "\n",
        "    criterion: nn.Module = FocalLoss(alpha=weights, gamma=2.0)\n",
        "\n",
        "    set_backbone_trainable(model, trainable=False)\n",
        "    optimizer: optim.Optimizer = build_optimizer_warmup(model, config)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "\n",
        "    current_phase = \"warmup\"\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    history: Dict[str, List[float]] = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_f1 = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        if current_phase == \"warmup\" and epoch == config.FREEZE_EPOCHS:\n",
        "            print(\"\\nUnfreezing backbone and switching to discriminative LRs...\")\n",
        "            set_backbone_trainable(model, trainable=True)\n",
        "            optimizer = build_optimizer_finetune(model, config)\n",
        "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
        "            current_phase = \"finetune\"\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
        "        val_loss, val_acc, val_f1 = validate(model, val_loader, criterion, config.DEVICE)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"\\nTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_f1': val_f1,\n",
        "                'val_loss': val_loss,\n",
        "            }, os.path.join(config.CHECKPOINT_DIR, 'best_model.pth'))\n",
        "            print(f\" Best model saved! Val F1: {val_f1:.4f}\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    plot_training_history(history, os.path.join(config.RESULTS_DIR, 'training_history.png'))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"CALIBRATING AND EVALUATING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    checkpoint = torch.load(os.path.join(config.CHECKPOINT_DIR, 'best_model.pth'), map_location=config.DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(config.DEVICE)\n",
        "\n",
        "    scaler = fit_temperature(model, val_loader, config.DEVICE, verbose=True)\n",
        "\n",
        "    print(\"\\nSelecting threshold on VALIDATION (calibrated)...\")\n",
        "    val_metrics_cal = evaluate_model(model, val_loader, config.DEVICE, scaler=scaler, threshold=None)\n",
        "    t_f1, val_f1_at_t = best_threshold_for_f1(val_metrics_cal['labels'], val_metrics_cal['probabilities'])\n",
        "    print(f\"Chosen threshold for F1: t={t_f1:.4f} (val F1 @ t = {val_f1_at_t:.4f})\")\n",
        "\n",
        "    print(\"\\nEvaluating UNCALIBRATED on test...\")\n",
        "    metrics_raw = evaluate_model(model, test_loader, config.DEVICE, scaler=None, threshold=None)\n",
        "\n",
        "    print(\"\\nEvaluating CALIBRATED (default argmax) on test...\")\n",
        "    metrics_cal = evaluate_model(model, test_loader, config.DEVICE, scaler=scaler, threshold=None)\n",
        "\n",
        "    print(\"\\nEvaluating CALIBRATED + THRESHOLD-TUNED on test...\")\n",
        "    metrics_cal_t = evaluate_model(model, test_loader, config.DEVICE, scaler=scaler, threshold=t_f1)\n",
        "\n",
        "    def _pretty(m: Dict[str, Any]) -> str:\n",
        "        return (f\"Acc {m['accuracy']:.4f} | Prec {m['precision']:.4f} | \"\n",
        "                f\"Rec {m['recall']:.4f} | F1 {m['f1']:.4f} | \"\n",
        "                f\"AUC {m['roc_auc']:.4f} | ECE {m['ece']:.4f} | \"\n",
        "                f\"Brier {m['brier']:.4f} | NLL {m['nll']:.4f}\")\n",
        "\n",
        "    print(\"\\nTest (uncalibrated, argmax):\")\n",
        "    print(_pretty(metrics_raw))\n",
        "    print(\"\\nTest (calibrated, argmax):\")\n",
        "    print(_pretty(metrics_cal))\n",
        "    print(\"\\nTest (calibrated, tuned t):\")\n",
        "    print(_pretty(metrics_cal_t))\n",
        "\n",
        "    plot_reliability_diagram(metrics_raw['labels'], metrics_raw['probabilities'],\n",
        "                             os.path.join(config.RESULTS_DIR, 'reliability_uncalibrated.png'))\n",
        "    plot_reliability_diagram(metrics_cal['labels'], metrics_cal['probabilities'],\n",
        "                             os.path.join(config.RESULTS_DIR, 'reliability_calibrated.png'))\n",
        "\n",
        "    plot_confusion_matrix(metrics_cal['confusion_matrix'],\n",
        "                          os.path.join(config.RESULTS_DIR, 'confusion_matrix_calibrated.png'))\n",
        "    plot_roc_curve(metrics_cal['labels'], metrics_cal['probabilities'],\n",
        "                   os.path.join(config.RESULTS_DIR, 'roc_curve_calibrated.png'))\n",
        "\n",
        "    print(f\"\\nResults saved to: {config.RESULTS_DIR}\")\n",
        "    print(f\"Temperature T: {scaler.T.item():.4f}\")\n",
        "    print(\"Training + calibration complete!\")\n",
        "\n",
        "    return model, metrics_cal, history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run training\n",
        "    model, metrics, history = main(use_ham10000=True, use_isic=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB2_XuMR58WJ",
        "outputId": "0e1d3c91-174f-4031-ba8c-636ede22eaae"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "MELANOMA CLASSIFICATION - TRAINING PIPELINE\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Model: senet50\n",
            "Image size: 224\n",
            "Batch size: 32\n",
            "Learning rate: 0.001\n",
            "======================================================================\n",
            "Loading HAM10000 dataset....\n",
            "Loaded 10015 images\n",
            "Melanoma: 1113\n",
            "Benign: 8902\n",
            "\n",
            "Class Distribution:\n",
            "dx\n",
            "nv       6705\n",
            "mel      1113\n",
            "bkl      1099\n",
            "bcc       514\n",
            "akiec     327\n",
            "vasc      142\n",
            "df        115\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Splitting dataset...\n",
            "Train: 5007 | Val: 2003 | Test: 3005\n",
            "\n",
            "Class weights: {0: np.float64(0.5624578746349135), 1: np.float64(4.502697841726619)}\n",
            "\n",
            "Creating senet50 model...\n",
            "\n",
            "Starting training...\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:59<00:00,  2.63it/s, loss=0.311, acc=25.2]\n",
            "Validation: 100%|| 63/63 [00:20<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.3108 | Train Acc: 0.2520\n",
            "Val Loss: 0.2307 | Val Acc: 0.2317\n",
            "LR: 0.001000\n",
            " Best model saved! Val F1: 0.2247\n",
            "\n",
            "Epoch 2/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:48<00:00,  3.22it/s, loss=0.284, acc=20.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2841 | Train Acc: 0.2077\n",
            "Val Loss: 0.2197 | Val Acc: 0.2062\n",
            "LR: 0.001000\n",
            "\n",
            "Epoch 3/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [00:48<00:00,  3.24it/s, loss=0.289, acc=21.9]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2889 | Train Acc: 0.2191\n",
            "Val Loss: 0.2218 | Val Acc: 0.1628\n",
            "LR: 0.001000\n",
            "\n",
            "Epoch 4/250\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Unfreezing backbone and switching to discriminative LRs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.237, acc=27.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2369 | Train Acc: 0.2792\n",
            "Val Loss: 0.1987 | Val Acc: 0.4019\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.2686\n",
            "\n",
            "Epoch 5/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.228, acc=36.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2276 | Train Acc: 0.3691\n",
            "Val Loss: 0.1908 | Val Acc: 0.4169\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.2727\n",
            "\n",
            "Epoch 6/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.222, acc=39.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2218 | Train Acc: 0.3992\n",
            "Val Loss: 0.1853 | Val Acc: 0.4373\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.2790\n",
            "\n",
            "Epoch 7/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.49it/s, loss=0.213, acc=41.5]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2134 | Train Acc: 0.4150\n",
            "Val Loss: 0.1878 | Val Acc: 0.4403\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.2809\n",
            "\n",
            "Epoch 8/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.49it/s, loss=0.208, acc=42.2]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2084 | Train Acc: 0.4216\n",
            "Val Loss: 0.1835 | Val Acc: 0.4853\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.2982\n",
            "\n",
            "Epoch 9/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.2, acc=43.5]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.2001 | Train Acc: 0.4354\n",
            "Val Loss: 0.1758 | Val Acc: 0.4433\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 10/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.199, acc=43.7]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1989 | Train Acc: 0.4366\n",
            "Val Loss: 0.1777 | Val Acc: 0.4753\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 11/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.49it/s, loss=0.193, acc=45]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1934 | Train Acc: 0.4504\n",
            "Val Loss: 0.1745 | Val Acc: 0.4578\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 12/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.195, acc=46.1]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1951 | Train Acc: 0.4608\n",
            "Val Loss: 0.1861 | Val Acc: 0.4893\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 13/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.194, acc=46.9]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1944 | Train Acc: 0.4691\n",
            "Val Loss: 0.1722 | Val Acc: 0.5127\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.3088\n",
            "\n",
            "Epoch 14/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.184, acc=48.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1836 | Train Acc: 0.4827\n",
            "Val Loss: 0.1636 | Val Acc: 0.5607\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.3313\n",
            "\n",
            "Epoch 15/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:04<00:00,  2.44it/s, loss=0.189, acc=49.5]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1888 | Train Acc: 0.4953\n",
            "Val Loss: 0.1667 | Val Acc: 0.5322\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 16/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.47it/s, loss=0.182, acc=49.5]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1824 | Train Acc: 0.4951\n",
            "Val Loss: 0.1677 | Val Acc: 0.5182\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 17/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:04<00:00,  2.43it/s, loss=0.181, acc=50.6]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1807 | Train Acc: 0.5065\n",
            "Val Loss: 0.1665 | Val Acc: 0.5282\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 18/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.45it/s, loss=0.183, acc=51.2]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1830 | Train Acc: 0.5123\n",
            "Val Loss: 0.1661 | Val Acc: 0.5242\n",
            "LR: 0.000100\n",
            "\n",
            "Epoch 19/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.47it/s, loss=0.182, acc=50.4]\n",
            "Validation: 100%|| 63/63 [00:16<00:00,  3.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1818 | Train Acc: 0.5045\n",
            "Val Loss: 0.1649 | Val Acc: 0.5776\n",
            "LR: 0.000100\n",
            " Best model saved! Val F1: 0.3370\n",
            "\n",
            "Epoch 20/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.179, acc=53.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1791 | Train Acc: 0.5345\n",
            "Val Loss: 0.1669 | Val Acc: 0.5237\n",
            "LR: 0.000050\n",
            "\n",
            "Epoch 21/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.174, acc=52.5]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1739 | Train Acc: 0.5249\n",
            "Val Loss: 0.1648 | Val Acc: 0.5402\n",
            "LR: 0.000050\n",
            "\n",
            "Epoch 22/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.46it/s, loss=0.174, acc=53.5]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1742 | Train Acc: 0.5349\n",
            "Val Loss: 0.1597 | Val Acc: 0.5826\n",
            "LR: 0.000050\n",
            " Best model saved! Val F1: 0.3397\n",
            "\n",
            "Epoch 23/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.47it/s, loss=0.166, acc=54.2]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1658 | Train Acc: 0.5418\n",
            "Val Loss: 0.1639 | Val Acc: 0.5931\n",
            "LR: 0.000050\n",
            " Best model saved! Val F1: 0.3454\n",
            "\n",
            "Epoch 24/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.167, acc=54.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1674 | Train Acc: 0.5430\n",
            "Val Loss: 0.1653 | Val Acc: 0.6111\n",
            "LR: 0.000050\n",
            " Best model saved! Val F1: 0.3525\n",
            "\n",
            "Epoch 25/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.172, acc=55.7]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1716 | Train Acc: 0.5570\n",
            "Val Loss: 0.1684 | Val Acc: 0.5956\n",
            "LR: 0.000050\n",
            "\n",
            "Epoch 26/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.167, acc=53.4]\n",
            "Validation: 100%|| 63/63 [00:16<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1673 | Train Acc: 0.5337\n",
            "Val Loss: 0.1654 | Val Acc: 0.5297\n",
            "LR: 0.000050\n",
            "\n",
            "Epoch 27/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.165, acc=55.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1649 | Train Acc: 0.5544\n",
            "Val Loss: 0.1700 | Val Acc: 0.5577\n",
            "LR: 0.000050\n",
            "\n",
            "Epoch 28/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.169, acc=54.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1692 | Train Acc: 0.5440\n",
            "Val Loss: 0.1702 | Val Acc: 0.6056\n",
            "LR: 0.000025\n",
            " Best model saved! Val F1: 0.3535\n",
            "\n",
            "Epoch 29/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.50it/s, loss=0.169, acc=54.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1688 | Train Acc: 0.5426\n",
            "Val Loss: 0.1585 | Val Acc: 0.6016\n",
            "LR: 0.000025\n",
            "\n",
            "Epoch 30/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.47it/s, loss=0.166, acc=55.6]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1658 | Train Acc: 0.5564\n",
            "Val Loss: 0.1612 | Val Acc: 0.5921\n",
            "LR: 0.000025\n",
            "\n",
            "Epoch 31/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.47it/s, loss=0.165, acc=54.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1645 | Train Acc: 0.5492\n",
            "Val Loss: 0.1606 | Val Acc: 0.5691\n",
            "LR: 0.000025\n",
            "\n",
            "Epoch 32/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.49it/s, loss=0.164, acc=55]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1636 | Train Acc: 0.5502\n",
            "Val Loss: 0.1650 | Val Acc: 0.5781\n",
            "LR: 0.000025\n",
            "\n",
            "Epoch 33/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.48it/s, loss=0.162, acc=56]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1621 | Train Acc: 0.5596\n",
            "Val Loss: 0.1615 | Val Acc: 0.5916\n",
            "LR: 0.000025\n",
            "\n",
            "Epoch 34/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.47it/s, loss=0.164, acc=56.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1638 | Train Acc: 0.5634\n",
            "Val Loss: 0.1664 | Val Acc: 0.5647\n",
            "LR: 0.000025\n",
            "\n",
            "Epoch 35/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:03<00:00,  2.46it/s, loss=0.166, acc=54.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1659 | Train Acc: 0.5486\n",
            "Val Loss: 0.1642 | Val Acc: 0.6116\n",
            "LR: 0.000013\n",
            "\n",
            "Epoch 36/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.53it/s, loss=0.164, acc=57.2]\n",
            "Validation: 100%|| 63/63 [00:16<00:00,  3.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1644 | Train Acc: 0.5720\n",
            "Val Loss: 0.1637 | Val Acc: 0.5991\n",
            "LR: 0.000013\n",
            "\n",
            "Epoch 37/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.52it/s, loss=0.16, acc=56.8]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1604 | Train Acc: 0.5684\n",
            "Val Loss: 0.1636 | Val Acc: 0.5801\n",
            "LR: 0.000013\n",
            "\n",
            "Epoch 38/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.54it/s, loss=0.154, acc=57.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1544 | Train Acc: 0.5744\n",
            "Val Loss: 0.1688 | Val Acc: 0.6316\n",
            "LR: 0.000013\n",
            " Best model saved! Val F1: 0.3616\n",
            "\n",
            "Epoch 39/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.51it/s, loss=0.158, acc=57.2]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1581 | Train Acc: 0.5718\n",
            "Val Loss: 0.1679 | Val Acc: 0.6111\n",
            "LR: 0.000013\n",
            "\n",
            "Epoch 40/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.49it/s, loss=0.158, acc=58.1]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1582 | Train Acc: 0.5806\n",
            "Val Loss: 0.1633 | Val Acc: 0.6051\n",
            "LR: 0.000013\n",
            "\n",
            "Epoch 41/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.55it/s, loss=0.161, acc=57.7]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1615 | Train Acc: 0.5772\n",
            "Val Loss: 0.1637 | Val Acc: 0.6026\n",
            "LR: 0.000006\n",
            "\n",
            "Epoch 42/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.54it/s, loss=0.156, acc=57.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1565 | Train Acc: 0.5782\n",
            "Val Loss: 0.1652 | Val Acc: 0.6016\n",
            "LR: 0.000006\n",
            "\n",
            "Epoch 43/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.52it/s, loss=0.154, acc=57.6]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1539 | Train Acc: 0.5762\n",
            "Val Loss: 0.1658 | Val Acc: 0.6216\n",
            "LR: 0.000006\n",
            "\n",
            "Epoch 44/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.51it/s, loss=0.154, acc=57.1]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1536 | Train Acc: 0.5706\n",
            "Val Loss: 0.1670 | Val Acc: 0.6355\n",
            "LR: 0.000006\n",
            " Best model saved! Val F1: 0.3652\n",
            "\n",
            "Epoch 45/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.53it/s, loss=0.155, acc=57.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1547 | Train Acc: 0.5738\n",
            "Val Loss: 0.1665 | Val Acc: 0.6321\n",
            "LR: 0.000006\n",
            "\n",
            "Epoch 46/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.53it/s, loss=0.16, acc=57.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  3.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1601 | Train Acc: 0.5732\n",
            "Val Loss: 0.1659 | Val Acc: 0.6256\n",
            "LR: 0.000006\n",
            "\n",
            "Epoch 47/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.60it/s, loss=0.158, acc=57.5]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1575 | Train Acc: 0.5750\n",
            "Val Loss: 0.1633 | Val Acc: 0.6121\n",
            "LR: 0.000003\n",
            "\n",
            "Epoch 48/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.57it/s, loss=0.159, acc=58.2]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1586 | Train Acc: 0.5822\n",
            "Val Loss: 0.1649 | Val Acc: 0.6146\n",
            "LR: 0.000003\n",
            "\n",
            "Epoch 49/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.55it/s, loss=0.157, acc=57.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1572 | Train Acc: 0.5736\n",
            "Val Loss: 0.1637 | Val Acc: 0.6066\n",
            "LR: 0.000003\n",
            "\n",
            "Epoch 50/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.156, acc=57.2]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1558 | Train Acc: 0.5724\n",
            "Val Loss: 0.1643 | Val Acc: 0.6071\n",
            "LR: 0.000003\n",
            "\n",
            "Epoch 51/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.59it/s, loss=0.159, acc=58.1]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1587 | Train Acc: 0.5814\n",
            "Val Loss: 0.1697 | Val Acc: 0.6460\n",
            "LR: 0.000003\n",
            " Best model saved! Val F1: 0.3698\n",
            "\n",
            "Epoch 52/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.58it/s, loss=0.154, acc=57.8]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1539 | Train Acc: 0.5780\n",
            "Val Loss: 0.1634 | Val Acc: 0.6141\n",
            "LR: 0.000003\n",
            "\n",
            "Epoch 53/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.55it/s, loss=0.154, acc=57.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1537 | Train Acc: 0.5782\n",
            "Val Loss: 0.1635 | Val Acc: 0.6066\n",
            "LR: 0.000002\n",
            "\n",
            "Epoch 54/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.54it/s, loss=0.157, acc=58]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1570 | Train Acc: 0.5798\n",
            "Val Loss: 0.1637 | Val Acc: 0.6146\n",
            "LR: 0.000002\n",
            "\n",
            "Epoch 55/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.16, acc=58.1]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1603 | Train Acc: 0.5808\n",
            "Val Loss: 0.1639 | Val Acc: 0.6141\n",
            "LR: 0.000002\n",
            "\n",
            "Epoch 56/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.152, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1519 | Train Acc: 0.5840\n",
            "Val Loss: 0.1637 | Val Acc: 0.6056\n",
            "LR: 0.000002\n",
            "\n",
            "Epoch 57/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.58it/s, loss=0.152, acc=58.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1516 | Train Acc: 0.5828\n",
            "Val Loss: 0.1620 | Val Acc: 0.6031\n",
            "LR: 0.000002\n",
            "\n",
            "Epoch 58/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.54it/s, loss=0.151, acc=58.2]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1506 | Train Acc: 0.5824\n",
            "Val Loss: 0.1656 | Val Acc: 0.6181\n",
            "LR: 0.000002\n",
            "\n",
            "Epoch 59/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.57it/s, loss=0.153, acc=57.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1535 | Train Acc: 0.5784\n",
            "Val Loss: 0.1659 | Val Acc: 0.6201\n",
            "LR: 0.000001\n",
            "\n",
            "Epoch 60/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.57it/s, loss=0.152, acc=58.2]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1516 | Train Acc: 0.5816\n",
            "Val Loss: 0.1650 | Val Acc: 0.6186\n",
            "LR: 0.000001\n",
            "\n",
            "Epoch 61/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.57it/s, loss=0.156, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1558 | Train Acc: 0.5840\n",
            "Val Loss: 0.1651 | Val Acc: 0.6206\n",
            "LR: 0.000001\n",
            "\n",
            "Epoch 62/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.155, acc=58.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1546 | Train Acc: 0.5884\n",
            "Val Loss: 0.1656 | Val Acc: 0.6266\n",
            "LR: 0.000001\n",
            "\n",
            "Epoch 63/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.59it/s, loss=0.151, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1508 | Train Acc: 0.5836\n",
            "Val Loss: 0.1634 | Val Acc: 0.6171\n",
            "LR: 0.000001\n",
            "\n",
            "Epoch 64/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.55it/s, loss=0.156, acc=58.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1556 | Train Acc: 0.5826\n",
            "Val Loss: 0.1628 | Val Acc: 0.6241\n",
            "LR: 0.000001\n",
            "\n",
            "Epoch 65/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.59it/s, loss=0.15, acc=58.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1505 | Train Acc: 0.5886\n",
            "Val Loss: 0.1671 | Val Acc: 0.6316\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 66/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.60it/s, loss=0.155, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1549 | Train Acc: 0.5842\n",
            "Val Loss: 0.1658 | Val Acc: 0.6231\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 67/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.60it/s, loss=0.159, acc=58.3]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1594 | Train Acc: 0.5828\n",
            "Val Loss: 0.1665 | Val Acc: 0.6226\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 68/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.59it/s, loss=0.157, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1570 | Train Acc: 0.5842\n",
            "Val Loss: 0.1649 | Val Acc: 0.6241\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 69/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.59it/s, loss=0.157, acc=58.9]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1567 | Train Acc: 0.5886\n",
            "Val Loss: 0.1649 | Val Acc: 0.6181\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 70/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.59it/s, loss=0.148, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1483 | Train Acc: 0.5842\n",
            "Val Loss: 0.1628 | Val Acc: 0.6026\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 71/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:00<00:00,  2.61it/s, loss=0.157, acc=58.7]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1567 | Train Acc: 0.5866\n",
            "Val Loss: 0.1661 | Val Acc: 0.6256\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 72/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.153, acc=58.6]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1533 | Train Acc: 0.5862\n",
            "Val Loss: 0.1668 | Val Acc: 0.6400\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 73/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.152, acc=58.2]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1524 | Train Acc: 0.5820\n",
            "Val Loss: 0.1656 | Val Acc: 0.6306\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 74/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.152, acc=58.5]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1522 | Train Acc: 0.5852\n",
            "Val Loss: 0.1642 | Val Acc: 0.6196\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 75/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.52it/s, loss=0.147, acc=59]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1466 | Train Acc: 0.5904\n",
            "Val Loss: 0.1640 | Val Acc: 0.6176\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 76/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.54it/s, loss=0.149, acc=59.2]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1487 | Train Acc: 0.5922\n",
            "Val Loss: 0.1638 | Val Acc: 0.6116\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 77/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:02<00:00,  2.53it/s, loss=0.157, acc=58.4]\n",
            "Validation: 100%|| 63/63 [00:15<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1570 | Train Acc: 0.5838\n",
            "Val Loss: 0.1652 | Val Acc: 0.6156\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 78/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.54it/s, loss=0.156, acc=58.2]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1559 | Train Acc: 0.5824\n",
            "Val Loss: 0.1631 | Val Acc: 0.6071\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 79/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.56it/s, loss=0.165, acc=58.3]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1648 | Train Acc: 0.5830\n",
            "Val Loss: 0.1637 | Val Acc: 0.6116\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 80/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.57it/s, loss=0.149, acc=58.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1489 | Train Acc: 0.5876\n",
            "Val Loss: 0.1673 | Val Acc: 0.6266\n",
            "LR: 0.000000\n",
            "\n",
            "Epoch 81/250\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|| 157/157 [01:01<00:00,  2.55it/s, loss=0.155, acc=58.8]\n",
            "Validation: 100%|| 63/63 [00:14<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Loss: 0.1554 | Train Acc: 0.5882\n",
            "Val Loss: 0.1660 | Val Acc: 0.6326\n",
            "LR: 0.000000\n",
            "\n",
            "Early stopping at epoch 81\n",
            "\n",
            "======================================================================\n",
            "CALIBRATING AND EVALUATING\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting logits for calibration: 100%|| 63/63 [00:15<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature learned: T=1.2216 | NLL: 0.6041 -> 0.6005\n",
            "\n",
            "Selecting threshold on VALIDATION (calibrated)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 63/63 [00:14<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chosen threshold for F1: t=0.6979 (val F1 @ t = 0.5027)\n",
            "\n",
            "Evaluating UNCALIBRATED on test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 94/94 [00:32<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating CALIBRATED (default argmax) on test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 94/94 [00:22<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating CALIBRATED + THRESHOLD-TUNED on test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|| 94/94 [00:22<00:00,  4.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test (uncalibrated, argmax):\n",
            "Acc 0.6556 | Prec 0.2386 | Rec 0.9581 | F1 0.3821 | AUC 0.8898 | ECE 0.3358 | Brier 0.2038 | NLL 0.5899\n",
            "\n",
            "Test (calibrated, argmax):\n",
            "Acc 0.6556 | Prec 0.2386 | Rec 0.9581 | F1 0.3821 | AUC 0.8898 | ECE 0.3426 | Brier 0.2022 | NLL 0.5885\n",
            "\n",
            "Test (calibrated, tuned t):\n",
            "Acc 0.8682 | Prec 0.4370 | Rec 0.6437 | F1 0.5206 | AUC 0.8898 | ECE 0.3426 | Brier 0.2022 | NLL 0.5885\n",
            "\n",
            "Results saved to: ./output/results\n",
            "Temperature T: 1.2216\n",
            "Training + calibration complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "hD9RbbROuGyL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}